\parindent=3em
\def\dts{\mathinner{\ldotp\ldotp}}
\def\para#1{\noindent\hbox to\parindent{\hss\bf#1 }}
\newcount\qcnt\qcnt=0
\def\A{\medskip\para{A\number\qcnt:}}
\def\Q{\bigskip\penalty-250\advance\qcnt by1\para{Q\number\qcnt:}}
\def\limp{\Rightarrow}

\Q Who uses FreeBoogie? What are the future plans for it?

\A OpenJML contains a copy of FreeBoogie, mainly for its Boogie AST data
structures. Two particularly glaring issues, which make it unusable as a
program verifier, are the unsound treatment of polymorphic types, and the
very unsound treatment of loops. I plan to fix these two problems by the
end of 2010. Once the number of bugs is reduced to an acceptable level, the
next two developments I interested in are (1)~symbolic execution and (2)~a
multi-prover backend.

\Q Can we use FreeBoogie instead of Boogie already?

\A No. There are unfortunately many wrinkles that need to be straightened.
A particularly annoying one is that the implementation of loop cutting has
a soundness bug. A complete list of known issues appears on FreeBoogie's
site at Google Code. So far, the development of FreeBoogie happened in
coding sprints. There was one in 2007, one in 2008, and one in 2009, each
about six weeks in length. The one for 2010 is overdue. It {\it must\/}
happen before Christmas, preferably earlier.

\Q Does FreeBoogie handle Boogie's polymorphic types?

\A It type-checks them, but does not encode their meaning in the
verification condition, which is known to be sometimes necessary. (In other
words, it is known that just erasing polymorphic Boogie types is unsound.)
[TODO: Example.]

\Q How does FreeBoogie handle loops?

\A It havocs all the variables written by a strong component at the entries
of that component, and then cuts all back-edges (with respect to an
arbitrary depth-first tree).  This was supposed to be sound even for
non-reducible flowgraphs, but it is not. Consider a program consisting of
two nested loops. Only the entry point of the outer loop is an entry point
for the (unique) strong component, so variables are {\it not\/} havocked at
the entry of the inner loop. In general, to cut a path $1\to\cdots\to
k\to\cdots\to n\to k$ we cut $n\to k$, assert an invariant~$I$ after~$n$,
insert before~$k$ a havoc of all variables assigned in $k\dts n$, and
assume~$I$ after the havoc. The trouble is that there might be more than
one such path, and we have to do the same for all, without actually looking
at all. The trick is to find a {\it loop forest\/} of the flowgraph that
has the following properties.
\item{1.} A {\it loop\/} is an induced subgraph that is strongly connected.
\item{2.} Two loops are either disjoint or nested.
\item{3.} Each cycle $k\to\cdots\to n\to k$ of the flowgraph is completely
contained in a loop and it touches at least one entry point of that loop.
A node is an {\it entry point\/} of a loop when it is reachable from the
initial node without using any other node of the loop.

\noindent Once we know a loop forest we can process it starting with the
innermost loops. There are multiple ways of doing so. My plan was to
experiment with duplicating each loop---one copy would stand for the first
iteration and one for an arbitrary iteration.

No matter what the method is, it assumes some invariants are available.  In
Boogie these are computed using abstract interpretation. This suggests a
different way of handling loops, namely to do symbolic execution instead of
cutting loops as preparation for verification condition generation.

The rather serious bug in FreeBoogie's handling of loops may bias the
experiments that compare the two methods of generating verification
conditions. However, there are few nested loops in the Boogie benchmark.
Even where there are nested loops, one may argue that they are (unsoundly)
cut in the {\it same\/} way for both verification condition generation
methods.

\Q Isn't it cheating to assume that loops have been dealt with when you
address code reachability?

\A To some extent it is. A Boogie program specifies a set of executions.
When partial correctness is checked by generating a verification condition,
loops are handled by cutting edges in the flowgraph, havocking variables,
and inserting assumptions and assertions of loop invariants in the proper
places, as described above. However, it is known that no algorithm can come
up with invariants such that the set of executions remains unchanged.
Usually we aim for soundness (retain all existing executions) and tolerate
incompleteness (we may add other executions). Hence, unreachable statements
may seem reachable once loops are handled. On one hand, it seems strange to
ignore the exact method used to handle loops, given that it may affect the
results of the reachability analysis. On the other hand, the results of the
partial correctness analysis are also affected, so there is no {\it new\/}
issue. In VC generation we treat loop handling as an orthogonal problem,
even if it is not, for engineering reasons. We treat it as orthogonal from
checking partial correctness, and we treat it as orthogonal from checking
semantic reachability.


\Q Your reachability analysis identifies four very different types of
problems.  Are these problems distinguished in error messages? If no, then
how can it be done?

\A First, the implementation in ESC/Java does not look for blockers as
described in the dissertation. (It does not split nodes in two as is done
on line~1 of Figure~7.9.) So there is no error message that points to
doomed code. When some code is unreachable, then ESC/Java distinguishes
between preconditions, postconditions, and assertions. This distinction
does {\it not\/} directly map to the more useful categories identified in
the dissertation (inconsistent specs, soundness bugs, dead code, and so
on).  It's still an open problem how to identify these categories
automatically, and it may be the case that some guesswork will be needed.


\Q How big is the codebase of FreeBoogie?

\A The jar file has $\approx1/2\rm\,MB$. This includes code generated by
AstGen, CLOPS, and ANTLR\null. There are about $10{,}000$~lines in the {\tt
src} directory, and about $24{,}000$~lines in the {\tt generated-src}
directory.

\Q Did you describe FreeBoogie's design in a peer-reviewed paper?

\A No. There is one in preparation now, based on Chapter~3.

\Q You say that ``fortunately, the dissertation does not stem from one big
contribution, but rather from many smaller ones that are related.'' I would
say this is {\it unfortunate}, because any set of articles with unimportant
contributions from some area of research can be described in the same way.
So, how is your thesis different from a set of unimportant articles glued
together, which are `related' only because they fit roughly the same
research area? What is the main contribution of your PhD?

\A When I start reading a book, I am often intimidated by its size.  My
statement merely tries to assure the reader that my dissertation may be
digested one small piece at a time. I think the chapters on passive forms,
incremental verification, and semantic reachability are largely
independent. Each of these chapters has some concrete contributions.  For
example, I prove that computing optimal passive forms of a certain kind is
NP-hard, I propose simplifying prover queries based on previous prover
queries, and I give a heuristic that enables checking semantic reachability
in practice.

All these results are based on the early chapters that give operational
semantics to a core of Boogie, show how it corresponds to Hoare logic used
on a flowgraph, and finally establish the link with predicate transformers.
Because it forms the basis of the later chapters, both by providing insight
and by offering a foundation for formal manipulations, I believe the study
of various semantics for a core of Boogie is the main contribution.


\Q Should verification condition generators use weakest precondition or
strongest postcondition?

\A Preferably, they should use both methods in parallel, if the machine has
at least two processing units. Otherwise, there doesn't seem to be much
difference from the point of view of performance, but it makes sense to
have the strongest postcondition method implemented if one wants to
eventually check for semantic reachability.

These observations are based mainly on experiments on the Boogie benchmark
that used FreeBoogie as a verification condition generator and Z3 as an SMT
solver. As such, there are a few caveats. First, it is not clear how
representative of real programs the Boogie benchmark is. Second, the
benchmark is further biased by FreeBoogie's unsound treatment of loops.
Third, FreeBoogie forgets triggers which significantly affect the prover
performance. Again, I believe both VCs should be affected in a similar way,
but this is far from clear.  Fourth, other SMT solvers may behave wildly
different than Z3.

In any case, if memory accesses from two instances of the prover are
known to be truly concurrent (which may be the case if there are multiple
CPUs, but usually isn't the case with multiple cores), then having the two
methods run in parallel is a good idea.

\Q The comparison between different types of semantics is pretty standard,
isn't it?

\A If it is, I am not aware of it. The main reference I used is the book
{\sl Programs, Recursion, and Unbounded Choice\/} by Wim Hesselink, from
1992. It uses ${\it wp}$ to define the semantics of a language, then
defines Hoare triples in terms of ${\it wp}$, and later briefly introduces
operational semantics and proves it agrees with~${\it wp}$.  This treatment
is different from the one in my dissertation. First, unlike Boogie,
Hesselink's language is structured. Second, in my case operational
semantics are the starting point, and Hoare triples are the meeting point,
while for Hesselink ${\it wp}$~is both the starting and the meeting point.

I am sure there are other comparisons between various semantics. I have not
seen any that is quite the same as mine. Hesselink says in his introduction
that ``we prefer the Floyd--Hoare method [which is] formalized in various
ways that are known under a number of different names: axiomatic semantics,
Hoare logic, dynamic logic, weakest precondition semantics, etc.'' My point
is that even if my comparison differs only in details from existing
comparisons, it is often the case that details matter when formalizing
semantics.


\Q You present a proof technique for showing the correctness of algorithms
that simplify verification conditions. What exactly are the properties an
algorithm needs in order to be able to apply this proof technique?

\A The technique handles algorithms that are implementations of a predicate
transformer ${\it prune}$ that takes an old OK predicate~$p$ and a new
predicate~$q$ and returns a simplified version of~$q$. A rather unimportant
technical point is that the text that presents this technique assumes that
``$p$~is OK'' means $|\lnot p|$ rather than~$|p|$, so ``simplified
version'' above means ``equisatisfiable''. Intuitively, the technique says
that you should check that ${\it prune}$ always returns a predicate whose
validity/satisfiability is between that of~$p$ and that of~$q$.

\Q The example you give in the introduction is quite algorithmic in nature.
Why do you think this is the kind of problem that ``program verifiers
ought to be able'' to handle in the future?

\A The example is almost random: I simply picked a short non-trivial
algorithm that wouldn't take long to explain. Program verifiers ought to
handle most programs that will be written and that were written.

I did have, however, a more precise but more subjective motivation in mind.
Researchers now use at least two attack angles. First, some focus on
numbers, some on concurrency, some on iteration and recursion, and some on
pointers.  Second, some pick particular algorithms and prove their
correctness formally. (The problem with the second approach is that it is
too tempting to tweak the proving formalism and the result is that we have
too many such formalisms with a too narrow focus.) It seems to me that
focusing on graph algorithms in general is a sensible direction to try.  It
is a tougher challenge to figure out how to handle a large class of graph
algorithms (as opposed to, say, one particular garbage collection
algorithm), but there might be an unexpected benefit: We might gain a
better understanding on how to handle pointers, which essentially form a
graph.


\Q You suggest that the front-end of program verifiers is uninteresting.
Why?

\A I only meant to say that {\it I\/} tend to prefer the kind of problems
one tackles in the backend. The reason is that I like few rules and complex
interactions more than I like many rules and few interactions. Of course, I
dislike many rules and many interactions.

\Q You say that `insights' lead to simpler and more efficient
implementations.  Which insight leads to which efficiency gain? Which
insight leads to which simplification?

\A Here are a few examples.

\item{$\bullet$} The definition of passive form in terms of a relation
between flowgraphs leads to the linear time passivation algorithm that
produces a minimum of extra variables. This algorithm is very simple.

\item{$\bullet$} The in detail discussion of semantics leads to a linear
time algorithm for constructing reachability queries. (The algorithm based
on the weakest precondition predicate transformer is quadratic.)

\Q What does `structural' mean in `structural operational semantics'?

\A In structural operational semantics, pieces of program are part of the
state, and how execution proceeds depends on the structure of those pieces.

\Q Why do you say that denotational semantics are not much used today?

\A I saw operational semantics, predicate transformers, and Hoare logic
much more often in recent papers.

\Q What is a Kripke structure?

\A A transition system. More precisely, a non-determinitstic automaton
together with a family of (atomic) properties, each property being a set of
automaton's states.

\Q Which SMT solver did you use?

\A Mostly Z3. I got used to Simplify's language when I implemented semantic
reachability in ESC/Java, so it was a natural first target for FreeBoogie's
backend. I used Z3 because it is a newer theorem prover that understands
Simplify's language.

\Q How does Boogie compare to Why?

\A I read about Why, but did not use it. I understand that it is, roughly,
a trimmed down version of ML\null. This suggests that there is more work to
do in the frontend and less in the backend. For example, {\it if\/} Why has
{\bf let} bindings but not assignments, then a passive form must be found
by the frontend.

\Q How does ACSL compare to JML?

\A ACSL is for C, while JML is for Java. ACSL is inspired by JML.
I do not know much about differences in details.

\Q There is a standard command language for SMT provers. Why do you say
this is prover dependent?

\A As far as I know the SMT command language is still a {\it proposal}, and
I'm not familiar with any prover that implements it. However, I did not
look into this matter for the past few months. [TODO]

\Q What are the differences between symbolic execution and abstract
interpretation?

\Q How come Kleene's algorithm for building regular expressions out of
automata and Gauss's algorithm for solving systems of linear equations are
$kij$ algorithms?

\Q What are the important design decisions and how did they affect
FreeBoogie?

\Q On what kind of formulas does {\it prune\/} work well?

\Q Why develop FreeBoogie when the Boogie tool from MSR is open source?

\Q Is parallel assignment in core Boogie? The grammar in Figure~2.2 says
``no,'' the typing rule in Figure~2.5 says ``yes.''

\A Parallel assignment is not handles by the semantics explicitly. The
typing rule is too optimistic.

\Q You say that ``Boogie does not facilitate reasoning about termination.''
What kind of features would make it easy to reason about termination?

\Q You say that FreeBoogie adds prover-dependent axioms. Does it use
multiple provers? Can you give an example of an axiom that is sent in one
way to one prover and another way to another prover?

\Q Does FreeBoogie handle triggers? Aren't the benchmarks for wp/sp
meaningless in the absence of triggers?

\Q The `unsharing' algorithm seems to be a very heavyweight solution to a
problem that has a simple engineering solution: communicate with the prover
using its API. How do you comment?

\Q Chapter~3 is very different from the others and makes the dissertation
heterogeneous. Is there any reason it should stay?

\Q Aren't there too many jokes?

\Q Is AstGen a contribution of you thesis? It seems that you simply
describe in too much detail some engineering effort.

\A [Note: Mention research on similar tools. Yes, it is a contribution, but
not central.]

\Q Why did you waste time developing AstGen instead of using some existing
infrastructure for developing compilers?

\Q You describe a very general type of visitors, which makes the
description hard to follow. Moreover, you only use `normal' visitors, so
the general description is not needed. Why did you do it?

\Q Is there anything novel about the way you use visitors?

\Q What is the difference between an interpretation of a formula and a
model of a formula?

\Q How does FreeBoogie's prover interface compare to that of Why?

\Q You say that Benton's technique of doing equivalence proofs fits well
with the Boogie language. Can you elaborate?

\Q Why does FreeBoogie not support the SMT language?

\A [When the backend was developed, there was no SMT command language.]

\Q How many users does FreeBoogie have? If none, then why?

\Q Why isn't reachability analysis implemented in FreeBoogie?

\Q There is one chapter about incremental verification and one chapter
about semantic reachability analysis. None is implemented in FreeBoogie. It
seems that the theoretical work is not sufficiently backed by experimental
evidence.  How do you comment?

\Q Can you characterize the programs for which the weakest precondition is
superpolynomial? What about the strongest postcondition?

\Q Can you shorten the background on computational complexity?

\A [I'd rather have {\it more\/} background in the other chapters.]

\Q Why is a passive form of~$G$ equivalent to~$G$? (Page~52 states that ``it
is easy to see''.)

\Q You say that $16\%$~of Boogie benchmarks use {\bf goto} in an
interesting way. Can you give an example?

\Q Why do you introduce the notion of {\it non-redundant\/} passive
form?

\A [See Theorem 2 and Conjecture 1.]

\Q Did you make any progress on the open problems related to passivation?
Is anyone else interested in these problems?

\Q The thesis seems more concerned with solving cute toy problems, rather
than solving real problems. Isn't this a waste of energy?

\Q How do {\it passive forms\/} relate to {\it dynamic single assignment\/}?

\Q Can you describe, briefly, how matching algorithms work?

\Q Why is the weakest precondition method complete? And the strongest
postcondition method?

\Q On page~72 you derive the strongest postcondition of {\bf assert}~$q$.
The precondition is~$a$, and every OK postcondition~$b$ must make $a\limp
(q\land b)$ valid. There is no solution unless $|a\limp q|$.  When there
are solutions, the strongest one is $a$, which is equivalent to~$a\land q$.
Why do you use the latter, more complicated expression in~(5.14)?

\A [Fewer cases in~(5.21).]

\Q Can you explain why $(v\gets q)\;p\not\equiv(v=q)\limp p$?

\Q Can you prove that $$|(v\gets q)\;p|=|(v\limp q)\limp p|$$ when $p$~is
monotonic in~$v$?

\Q You argue that ${\it vc}_{\rm sp}$ is ``especially amenable'' to
splitting because it is a big conjunction. However, this seems like a
superficial advantage, because the conjoined parts do have lots of shared
subparts.  Is there any merit in your appreciation?

\Q Why do you spell out simple proofs in so much detail?

\Q The whole thesis describes a purely engineering effort to improve an
existing program. Moreover, in many cases the engineering effort is merely
planed, rather than carried out. What makes you think this work is worthy
of a PhD?

\Q In Section~6.3 you say that SMT solvers search for a `store', but, as
far as I know, SMT solvers are not familiar with the concept of a `store'.
Can you explain what you meant?

\Q How would a {\it prune\/} predicate look like in order to take advantage
of the VC shapes generated by the weakest precondition method?

\Q It is unclear what is the problem addressed in Section~6.4. What is the
point of getting rid of decorated names if the decoration is needed?

\A [Well, the decoration is not strictly speaking needed. It is just a
convenient way to explain (and implement) how the answer of the prover is
interpreted. In a way, we maintain that convenience, without sacrificing
speed.]

\Q The `proof technique' of Chapter~6 is straightforward. Why is it
presented as an important contribution of the dissertation?

\A [The first thing you'd try is different. Maybe the `proof technique' is
obvious only with hindsight?]

\Q Could you clarify what is the idea of `edit\&verify' and what is
implemented? Could you clarify what you hope may be achieved by
implementing the idea fully? Why did you not do so?

\Q You say that prunning is not worthwhile for easy queries. But how do you
know which queries are easy?

\A [(1)~You remember previous response times. (2)~I hope it's good on
average.]

\Q Is efficiency a primary goal of FreeBoogie or not? On one hand you choose
to typecheck the Boogie AST several time, on the other you run after small
improvements in speed in several chapters.

\A [It is, but it comes after correctness. The idea is to keep its code
as obviously correct as possible, by doing plenty of sanity checks, by
splitting tasks into very small subtasks, by tackling incrementality in
{\it one\/} place only, etc.]

\Q How does your work on edit\&verify compare with the work of Beckert and
Klebanov?

\Q Is edit\&verify useless if identifiers aren't decorated (say, because of
an expressive SMT command language)?

\Q The experiments were done using several tools---Spec$^\sharp$, ESC/Java,
FreeBoogie, Fx7, Simplify. Is there any place in the dissertation that
describes the relations between these? Is there a reason you didn't stick
to one or two tools?

\Q In Figure~7.7 you give a flowgraph in which all non-initial nodes are
direct children of the initial node. If the postcondition of the initial
node is satisfiable, then all nodes are semantically reachable. Therefore,
one prover query is enough to establish that we are indeed in the common
case with no bugs. However, the algorithm in Figure~7.9, despite being
designed to be fast in the common case with no bugs, will make one query
for each non-initial node. What is the problem?

\A [We also want to detect doomed code.]

\Q What kind of doomed assertions did you find in JavaFE (see page~120)?

\Q It is bothersome that 1-out-of-5 warnings given by reachability analysis
have no known cause. What makes you confident that the problem isn't
reachability analysis itself?

\Q Why not use abstract interpretation or symbolic execution to detect
semantically unreachable code?

\A [Separation of concerns: loops first, then other problems. Since loops
are such a central problem, I'm not really convinced that such separation
of concerns is a good idea, but I've inherited it from ESC/Java. I do plan
to do semantic reachability analysis in jStar.]

\Q What exactly is the relation between {\it unreachable code\/} and {\it
doomed code\/} on one hand and Lermer's {\it dead paths\/} and {\it dead
commands\/} on the other hand?

\Q What do you mean by ``well-founded specification'' (see page~121)?

\bye
% vim:textwidth=75:
