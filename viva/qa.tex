\parindent=3em
\def\para#1{\noindent\hbox to\parindent{\hss\bf#1 }}
\newcount\qcnt\qcnt=0
\def\A{\medskip\para{A\number\qcnt:}}
\def\Q{\bigskip\penalty-250\advance\qcnt by1\para{Q\number\qcnt:}}
\def\limp{\Rightarrow}

\Q Can we use FreeBoogie instead of Boogie already?

\A No. There are unfortunately many wrinkles that need to be straightened.
A particularly annoying one is that the implementation of loop cutting has
a soundness bug. A complete list of known issues appears on FreeBoogie's
site at Google Code. So far, the development of FreeBoogie happened in
coding sprints. There was one in 2007, one in 2008, and one in 2009, each
about six weeks in length. The one for 2010 is overdue. It {\it must\/}
happen before Christmas, preferably earlier.

\Q What is the main contribution of your PhD?

\A I believe that the main contribution of the thesis is the formal
development of several concepts (flowgraph of a Boogie program, weakest
precondition, etc.)\ that naturally leads to solutions of several problems
related to verification condition generation. None of these concepts is
new, and there are many textbooks that discuss them. However, I have not
seen any comprehensive treatment that relates flowgraphs, operational
semantics, Hoare logic, and predicate transformers in the way it is done in
my dissertation.  I believe it is this synthesis that led to `obvious'
solutions to several problems.

Let me be more explicit about the problems I'm talking about.  One problem
is how to analyse a program for semantic reachability.  Chapter~7 shows how
to do this faster than checking partial correctness.  This result should
have the greatest practical impact, if properly disseminated. Another
problem is finding the complexity of finding a passive form. The proof that
finding copy-optimal increasing-version passive forms is the most
technically difficult part of the dissertation. Yet another problem is
doing incremental verification, with a very fine granularity.

Finally, I want to mention that my PhD work resulted in a dissertation {\it
and\/} a tool, FreeBoogie. Its goal is to be a drop-in replacement for
MSR's Boogie tool, which is easier to maintain and build upon.


\Q Does FreeBoogie handle loops?

\A The current implementation simply cuts back-edges (for {\it some\/}
depth-first traversal), which is unsound. I am not sure what is the best
way to handle loops. MSR's Boogie uses abstract interpretation to infer
loop invariants. These are inserted at loop heads when back-edges are cut,
and these operations assume reducible flowgraphs. FreeBoogie could take the
same approach. Non-reducible graphs may be handled with some care. First,
one needs to pick a definition for `loop' (there is not only {\it one\/}
natural definition, as is the case for reducible flowgraphs); second, it
seems that one needs to duplicate the body of loops in order to handle the
`generic' and nested loops. This may lead to poor performance in the common
case that the flowgraph {\it is\/} reducible.

This potential performance problem is {\it not\/} why I'm unsure that this
is the best solution. What nags me is that loop handling seems such a
fundamental problem that it is hard to consider it as orthogonal to the
others (handling assignments, checking partial correctness, checking
semantic reachability). To put it plainly, it may be better for FreeBoogie
to do symbolic execution rather than verification condition generation.
There could be to flavours of symbolic execution, one going forward and one
backwards, corresponding to the two methods used for computing the
verification condition. Only forwards execution would be suitable for
semantic reachability analysis.

Does the unsound treatment of loops invalidate the results of experiments
carried out with FreeBoogie? I believe this is not the case for to reasons.
First, many programs (implementations) in the Boogie benchmark do not have
loops. Second, even if they do have loops, these are cut in the same way,
so the {\it comparison\/} between the two VCGEN methods stands.


\Q Did you describe FreeBoogie's design in a peer-reviewed paper?

\A No. There is one in preparation now, based on Chapter~3.

\Q Should verification condition generators use weakest precondition or
strongest postcondition?

\A Preferably, they should use both methods in parallel, if the machine has
at least two processing units. Otherwise, there doesn't seem to be much
difference from the point of view of performance, but it makes sense to
have the strongest postcondition method implemented if one wants to
eventually check for semantic reachability.

These observations are based mainly on experiments on the Boogie benchmark
that used FreeBoogie as a verification condition generator and Simplify as
an SMT solver. As such, there are a few caveats. First, it is not clear how
representative of real programs the Boogie benchmark is. Second, the
benchmark is further biased by FreeBoogie's unsound treatment of loops.
Third, FreeBoogie forgets triggers which significantly affect the prover
performance. Again, I believe both VCs should be affected in a similar way,
but this is far from clear.  Fourth, better SMT solvers may behave wildly
different than Simplify.

In any case, if memory accesses from two instances of the prover are
known to be truly concurrent (which may be the case if there are multiple
CPUs, but usually isn't the case with multiple cores), then having the two
methods run in parallel is a good idea.

\Q The comparison between different types of semantics is pretty standard,
isn't it?

\A It seems that is {\it should\/} be. The textbook I used for these things
is Hesselink, {\sl Programming and Recursion}, which is fairly recent. The
semantics there is defined in terms of~${\it wp}$. When Hoare triples (and
`annotations' in general) are introduced they are related to~${\it wp}$.
Later, operational semantics are briefly mentioned and they too are related
to~${\it wp}$. In summary, Hoare triples are related to~${\it wp}$, which
is related to operational semantics. That textbook states that the relation
it gives between ${\it wp}$ and operational semantics is new, arguing that
`axiomatic' people and `operational' people tend to dislike each-other
methods.

In my dissertation, operational semantics define the Boogie language, and
Hoare triples occupy a central place, being related to the other types of
semantics. To my knowledge, this approach to relating operational semantics
to~${\it wp}$ is new. It may turn out that the connection was already
described explicitly in these terms. Even if it wasn't, since such basic
concepts are involved, it is likely that it was `floating in the air' as a
sort of folklore knowledge that was never made quite explicit. If this is
the case, then I'm happy to have written down in detail.

\Q You present a proof technique for showing the correctness of algorithms
that simplify verification conditions. What exactly are the properties an
algorithm needs in order to be able to apply this proof technique?

\A The technique handles algorithms that are implementations of a predicate
transformer ${\it prune}$ that takes an old OK predicate~$p$ and a new
predicate~$q$ and returns a simplified version of~$q$. A rather unimportant
technical point is that the text that presents this technique assumes that
``$p$~is OK'' means $|\lnot p|$ rather than~$|p|$, so ``simplified
version'' above means ``equisatisfiable''. Intuitively, the technique says
that you should check that ${\it prune}$ always returns a predicate whose
validity/satisfiability is between that of~$p$ and that of~$q$.

\Q Does FreeBoogie handle Boogie's polymorphic types?

\A It type-checks, but does not encode the meaning in logic, which is known
to be sometimes necessary. (In other words, it is known that just erasing
types is unsound.)

\Q Isn't it cheating to assume that loops have been dealt with when you
address code reachability?

\A The cheeky answer is ``yes, but it's not {\it my\/} cheating''. I hope
the answer will seem less cheeky once I explain what I mean. Let me first
explain why it {\it is\/} cheating. A Boogie program can be seen as a way
to specify a set of executions. If edges are simply cut to eliminate loops,
then the set of executions is reduced, and it is possible for wrong
executions to be trimmed out. In other words, simply cutting edges is
unsound. To enlarge the set of executions we may forget what we know about
some variable at certain points (think of loop heads). If the variable
is~$x$, we do this by simply using~$x'$ in subsequent statements. That way,
whatever we knew about~$x$ does not apply to~$x'$. So that we don't
introduce too many new execution we may relate $x'$ to~$x$ using an
assumption. The relation is weaker than $x'=x$. Loop invariants play
exactly this role. Anyway, the point is that coming up with invariants is
hard, and often you must settle for the possibility that the loop-less
program has {\it more\/} executions than the loop-full program it is
supposed to be equivalent to. One problem is that wrong executions may be
introduced. Another problem, relevant here, is that statements that were
semantically unreachable may become semantically reachable.  So, assuming
that loops have been dealt with is cheating because (1)~we know that for
any method of handling loops there are programs where truly unreachable
statements are turned in reachable statements, and (2)~if you study
reachability it seems strange to ignore which exactly is the method used to
handle loops.

Now here's why it is {\it not\/} cheating. The {\it same\/} problem holds
for VC generation in general, as explained above. One may argue that it is
strange to talk about the wp or the sp method of generating VCs and
ignoring the precise method used to handle loops, given that any such
method {\it will\/} introduce wrong executions for {\it some\/} correct
input programs. Should this stop us from using VC generation completely and
settle for symbolic execution? It is certainly a serious reason, but it is
not that clear cut that the engineering advantages gained from separating
the concern of handling loops aren't more important.

\Q Your reachability analysis identifies four very different types of
problems.  Are these problems distinguished in error messages? If no, then
how can it be done?

\A First, the implementation in ESC/Java does not look for blockers as
described in the dissertation. (It does not split nodes in two as is done
on line~1 of Figure~7.9.) So there is no error message that points to
doomed code. When some code is unreachable, then ESC/Java distinguishes
between preconditions, postconditions, and assertions. This distinction
does {\it not\/} directly map to the more useful categories identified in
the dissertation (inconsistent specs, soundness bugs, dead code, and so
on).  It's still an open problem how to identify these categories
automatically, and it may be the case that some guesswork will be needed.

\Q The example you give in the introduction is quite algorithmic in nature.
Why do you think this is the kind of problems that ``program verifiers
ought to be able'' to handle ``in the future''?

\Q You suggest that the front-end of program verifiers is uninteresting.
Why?

\Q You say that `insights' lead to simpler and more efficient
implementations.  Which insight leads to which efficiency gain? Which
insight leads to which simplification?

\Q What does `structural' mean in `structural operational semantics'?
(Does it describe `operational semantics' or does it restrict to a kind?)

\Q Why do you say that denotational semantics are not much used today?

\Q What is a Kripke structure?

\Q Which SMT solver did you use?

\Q How does Boogie compare to Why?

\Q How does ACSL compare to JML?

\Q There is a standard command language for SMT provers. Why do you say
this is prover dependent?

\Q Who uses FreeBoogie? What are the future plans for it?

\Q How big is the codebase of FreeBoogie?

\Q What {\it are\/} the differences between symbolic execution and abstract
interpretation?

\Q How come Kleene's algorithm for building regular expressions out of
automata and Gauss's algorithm for solving systems of linear equations are
$kij$ algorithms?

\Q You say that ``fortunately, the dissertation does not stem from one big
contribution, but rather from many smaller ones that are related.'' I would
say this is {\it unfortunate}, because any set of articles with unimportant
contributions from some area of research can be described in the same way.
So, how is your thesis different from a set of unimportant articles glued
together, which are `related' only because they fit roughly the same
research area?

\Q What are the important design decisions and how did they affect
FreeBoogie?

\Q On what kind of formulas does {\it prune\/} work well?

\Q Why develop FreeBoogie when the Boogie tool from MSR is open source?

\Q Is parallel assignment in core Boogie? The grammar in Figure~2.2 says
``no,'' the typing rule in Figure~2.5 says ``yes.''

\A Parallel assignment is not handles by the semantics explicitly. The
typing rule is too optimistic.

\Q You say that ``Boogie does not facilitate reasoning about termination.''
What kind of features would make it easy to reason about termination?

\Q You say that FreeBoogie adds prover-dependent axioms. Does it use
multiple provers? Can you give an example of an axiom that is sent in one
way to one prover and another way to another prover?

\Q Does FreeBoogie handle triggers? Aren't the benchmarks for wp/sp
meaningless in the absence of triggers?

\Q The `unsharing' algorithm seems to be a very heavyweight solution to a
problem that has a simple engineering solution: communicate with the prover
using its API. How do you comment?

\Q Chapter~3 is very different from the others and makes the dissertation
heterogeneous. Is there any reason it should stay?

\Q Aren't there too many jokes?

\Q Is AstGen a contribution of you thesis? It seems that you simply
describe in too much detail some engineering effort.

\A [Note: Mention research on similar tools. Yes, it is a contribution, but
not central.]

\Q Why did you waste time developing AstGen instead of using some existing
infrastructure for developing compilers?

\Q You describe a very general type of visitors, which makes the
description hard to follow. Moreover, you only use `normal' visitors, so
the general description is not needed. Why did you do it?

\Q Is there anything novel about the way you use visitors?

\Q What is the difference between an interpretation of a formula and a
model of a formula?

\Q How does FreeBoogie's prover interface compare to that of Why?

\Q You say that Benton's technique of doing equivalence proofs fits well
with the Boogie language. Can you elaborate?

\Q Why does FreeBoogie not support the SMT language?

\A [When the backend was developed, there was no SMT command language.]

\Q How many users does FreeBoogie have? If none, then why?

\Q Why isn't reachability analysis implemented in FreeBoogie?

\Q There is one chapter about incremental verification and one chapter
about semantic reachability analysis. None is implemented in FreeBoogie. It
seems that the theoretical work is not sufficiently backed by experimental
evidence.  How do you comment?

\Q Can you characterize the programs for which the weakest precondition is
superpolynomial? What about the strongest postcondition?

\Q Can you shorten the background on computational complexity?

\A [I'd rather have {\it more\/} background in the other chapters.]

\Q Why is a passive form of~$G$ equivalent to~$G$? (Page~52 states that ``it
is easy to see''.)

\Q You say that $16\%$~of Boogie benchmarks use {\bf goto} in an
interesting way. Can you give an example?

\Q Why do you introduce the notion of {\it non-redundant\/} passive
form?

\A [See Theorem 2 and Conjecture 1.]

\Q Did you make any progress on the open problems related to passivation?
Is anyone else interested in these problems?

\Q The thesis seems more concerned with solving cute toy problems, rather
than solving real problems. Isn't this a waste of energy?

\Q How do {\it passive forms\/} relate to {\it dynamic single assignment\/}?

\Q Can you describe, briefly, how matching algorithms work?

\Q Why is the weakest precondition method complete? And the strongest
postcondition method?

\Q On page~72 you derive the strongest postcondition of {\bf assert}~$q$.
The precondition is~$a$, and every OK postcondition~$b$ must make $a\limp
(q\land b)$ valid. There is no solution unless $|a\limp q|$.  When there
are solutions, the strongest one is $a$, which is equivalent to~$a\land q$.
Why do you use the latter, more complicated expression in~(5.14)?

\A [Fewer cases in~(5.21).]

\Q Can you explain why $(v\gets q)\;p\not\equiv(v=q)\limp p$?

\Q Can you prove that $$|(v\gets q)\;p|=|(v\limp q)\limp p|$$ when $p$~is
monotonic in~$v$?

\Q You argue that ${\it vc}_{\rm sp}$ is ``especially amenable'' to
splitting because it is a big conjunction. However, this seems like a
superficial advantage, because the conjoined parts do have lots of shared
subparts.  Is there any merit in your appreciation?

\Q Why do you spell out simple proofs in so much detail?

\Q The whole thesis describes a purely engineering effort to improve an
existing program. Moreover, in many cases the engineering effort is merely
planed, rather than carried out. What makes you think this work is worthy
of a PhD?

\Q In Section~6.3 you say that SMT solvers search for a `store', but, as
far as I know, SMT solvers are not familiar with the concept of a `store'.
Can you explain what you meant?

\Q How would a {\it prune\/} predicate look like in order to take advantage
of the VC shapes generated by the weakest precondition method?

\Q It is unclear what is the problem addressed in Section~6.4. What is the
point of getting rid of decorated names if the decoration is needed?

\A [Well, the decoration is not strictly speaking needed. It is just a
convenient way to explain (and implement) how the answer of the prover is
interpreted. In a way, we maintain that convenience, without sacrificing
speed.]

\Q The `proof technique' of Chapter~6 is straightforward. Why is it
presented as an important contribution of the dissertation?

\A [The first thing you'd try is different. Maybe the `proof technique' is
obvious only with hindsight?]

\Q Could you clarify what is the idea of `edit\&verify' and what is
implemented? Could you clarify what you hope may be achieved by
implementing the idea fully? Why did you not do so?

\Q You say that prunning is not worthwhile for easy queries. But how do you
know which queries are easy?

\A [(1)~You remember previous response times. (2)~I hope it's good on
average.]

\Q Is efficiency a primary goal of FreeBoogie or not? On one hand you choose
to typecheck the Boogie AST several time, on the other you run after small
improvements in speed in several chapters.

\A [It is, but it comes after correctness. The idea is to keep its code
as obviously correct as possible, by doing plenty of sanity checks, by
splitting tasks into very small subtasks, by tackling incrementality in
{\it one\/} place only, etc.]

\Q How does your work on edit\&verify compare with the work of Beckert and
Klebanov?

\Q Is edit\&verify useless if identifiers aren't decorated (say, because of
an expressive SMT command language)?

\Q The experiments were done using several tools---Spec$^\sharp$, ESC/Java,
FreeBoogie, Fx7, Simplify. Is there any place in the dissertation that
describes the relations between these? Is there a reason you didn't stick
to one or two tools?

\Q In Figure~7.7 you give a flowgraph in which all non-initial nodes are
direct children of the initial node. If the postcondition of the initial
node is satisfiable, then all nodes are semantically reachable. Therefore,
one prover query is enough to establish that we are indeed in the common
case with no bugs. However, the algorithm in Figure~7.9, despite being
designed to be fast in the common case with no bugs, will make one query
for each non-initial node. What is the problem?

\A [We also want to detect doomed code.]

\Q What kind of doomed assertions did you find in JavaFE (see page~120)?

\Q It is bothersome that 1-out-of-5 warnings given by reachability analysis
have no known cause. What makes you confident that the problem isn't
reachability analysis itself?

\Q Why not use abstract interpretation or symbolic execution to detect
semantically unreachable code?

\A [Separation of concerns: loops first, then other problems. Since loops
are such a central problem, I'm not really convinced that such separation
of concerns is a good idea, but I've inherited it from ESC/Java. I do plan
to do semantic reachability analysis in jStar.]

\Q What exactly is the relation between {\it unreachable code\/} and {\it
doomed code\/} on one hand and Lermer's {\it dead paths\/} and {\it dead
commands\/} on the other hand?

\Q What do you mean by ``well-founded specification'' (see page~121)?

\bye
% vim:textwidth=75:
